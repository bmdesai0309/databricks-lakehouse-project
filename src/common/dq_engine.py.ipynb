{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f27cd9-0693-4ee3-943f-5687f107a903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67d3a4f-ae3b-4097-9fc3-fafb192e81b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataQualityEngine:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes engine with the JSON config.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.rules = config.get(\"data_quality_rules\", [])\n",
    "        self.source_name = config.get(\"source_name\", \"unknown_source\")\n",
    "    \n",
    "    def run_validation(self, df):\n",
    "        total_count = df.count()\n",
    "        if total_count == 0:\n",
    "            print(\"Source data is empty. Skipping validation.\")\n",
    "            return True, None\n",
    "        \n",
    "        print(f\"Validating {total_count} records for '{self.source_name}'...\")\n",
    "\n",
    "        # We will collect results in a list of dicts to show a summary table\n",
    "        dq_summary = []\n",
    "        is_all_valid = True\n",
    "\n",
    "        for rule in self.rules:\n",
    "            rule_name = rule['name']\n",
    "            condition = rule['expectation']\n",
    "\n",
    "            failed_df = df.filter(f\"NOT ({condition})\")\n",
    "            fail_count = failed_df.count()\n",
    "\n",
    "            status = \"PASS\" if fail_count == 0 else \"FAIL\"\n",
    "            if fail_count > 0:\n",
    "                is_all_valid = False\n",
    "            \n",
    "            dq_summary.append({\n",
    "                \"rule_name\": rule_name,\n",
    "                \"expectation\": condition,\n",
    "                \"fail_count\": fail_count,\n",
    "                \"pass_percentage\": ((total_count - fail_count) / total_count) * 100,\n",
    "                \"status\": status\n",
    "            })\n",
    "\n",
    "        summary_df = spark.createDataFrame(dq_summary)\n",
    "        return is_all_valid, summary_df\n",
    "    \n",
    "    def get_table_name(self, layer):\n",
    "        uc = self.config[\"unity_catalog\"]\n",
    "        return f\"{uc['catalog']}.{uc['schema']}.{uc[f'{layer}_table']}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3ece4f0-5a42-4a25-81d6-fca95112e6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dq_engine.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
